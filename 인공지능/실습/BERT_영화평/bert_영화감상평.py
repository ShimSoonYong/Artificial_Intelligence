# -*- coding: utf-8 -*-
"""BERT_영화감상평.ipynb

Automatically generated by Colab.

"""

# @title 모듈 임포트

import numpy as np
import torch
from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,
 TensorDataset)
from torch.utils.data.distributed import DistributedSampler
from tqdm import tqdm, trange
from torch import nn
import torch.nn.functional as F
import torch.optim
from torch.utils.data import Dataset, DataLoader
import time
from sklearn.metrics import classification_report
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from transformers import AutoModel, AutoTokenizer
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# @title BERT 로드
from huggingface_hub import login
from google.colab import userdata
userdata.get('HF_TOKEN')

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
bert = AutoModel.from_pretrained("bert-base-uncased")

# @title 워크플로우 내 상수 선언

batch_size = 64
best_acc = 0.0
num_epochs = 5
max_seq_leng = 256

from pickle import STRING
# @title 영화평 파일 로드 함수

def prepare_input_tok_seq_and_mask_seq_from_file (filename:str)->tuple:
    """Reads a file, tokenizes sentences, and creates input sequences and attention masks.

    This function reads a file containing movie reviews, tokenizes each sentence using the
    pre-trained BERT tokenizer, and creates input token sequences and attention masks
    suitable for feeding into a BERT model.

    Args:
        filename (str): The path to the file containing the movie reviews.
            The file is expected to have one review per line, with the format:
            "<label>\\t<review text>\\n".

    Returns:
        tuple: A tuple containing three NumPy arrays:
            - list_tok_seq: Input token sequences for each review.
            - list_mask_seq: Attention masks for each review.
            - target_labels: Target labels (0 or 1) for each review.
    """
    sentences = []
    target_labels = []
    # assume that file is in the program directiory (files are naver sentiment data files).
    with open(filename, "r", encoding="utf-8") as fpr:
        i=0
        for line in fpr.readlines():
            line_splited = line.split('\t') # the result is a list of 3 strings: doc_id, sentence, label.
            sent = line_splited[1]
            sent = sent[:-1]        #remove newline char
            sentences.append(sent)
            lab_str = line_splited[0]  # the last char is a tab.
            lab_idx = int(lab_str)
            target_labels.append(lab_idx)
            i = i+1
            if i % 5000 == 0:
                print("number of lines read from file = ", i)
    # each sentence string in sentences is converted into a list of token ids (indices).
    sentences_id = []  # a list of sublists; a sublist is a list of indexes of tokens in a sentence.
    for i, sentence in enumerate(sentences):
        token_list = tokenizer.tokenize(sentence)
        token_id_list = tokenizer.convert_tokens_to_ids(token_list)
        sentences_id.append(token_id_list)
        if i % 5000 == 0:
            print("tokenizing sentences i", i)
    # prepare 2 separate lists having input token sequence and attention mask.
    list_tok_seq = []
    list_mask_seq = []
     # 앞 페이지의 list_mask_seq 문장과 같은 깊이임.
    for k, token_list in enumerate(sentences_id):
        if k % 5000 == 0:
            print("Number of reviews (for training examples) processed = ", k)
        tok_seq = np.zeros(max_seq_leng, dtype=np.int_)  # this is for one sentence.
        mask_seq = np.zeros(max_seq_leng, dtype=np.int_)
        tok_seq[0] = 2  # add idx of [CLS] as first element
        mask_seq[0] = 1
        tk_leng = len(token_list)
        if tk_leng > max_seq_leng - 2:
            tk_leng1 = max_seq_leng - 2
        else:
            tk_leng1 = tk_leng
        place_sep = tk_leng1 + 1
        for i in range(tk_leng1):
            tok_seq[i + 1] = token_list[i]
            mask_seq[i + 1] = 1
        tok_seq[place_sep] = 3  # idx of token [SEP]
        mask_seq[place_sep] = 1

        if place_sep < max_seq_leng - 1:
            for j in range(place_sep + 1, max_seq_leng):
                tok_seq[j] = 0  # add pad.
                mask_seq[j] = 0  # add 0 mask.
        list_tok_seq.append(tok_seq)
        list_mask_seq.append(mask_seq)

    list_tok_seq = np.array(list_tok_seq, np.int32)
    list_mask_seq = np.array(list_mask_seq, np.int32)
    target_labels = np.array(target_labels, np.int32)

    return list_tok_seq, list_mask_seq, target_labels

# @title 영화평 데이터 로드 및 전처리

list_tok_seq_all, list_mask_seq_all, target_labels_all = \
    prepare_input_tok_seq_and_mask_seq_from_file("./train_data_movie_reviews_imdb.txt")

# 데이터 개수 계산
total_num_examples = list_tok_seq_all.shape[0]
num_train_examples = int(total_num_examples * 0.8)
num_val_examples = int(total_num_examples * 0.1)
num_test_examples = total_num_examples - num_train_examples - num_val_examples

# 학습, 검증, 테스트 데이터 분리
list_tok_seq_train, list_mask_seq_train, target_labels_train = \
    list_tok_seq_all[:num_train_examples], list_mask_seq_all[:num_train_examples], target_labels_all[:num_train_examples]
list_tok_seq_val, list_mask_seq_val, target_labels_val = \
    list_tok_seq_all[num_train_examples:num_train_examples+num_val_examples], list_mask_seq_all[num_train_examples:num_train_examples+num_val_examples], target_labels_all[num_train_examples:num_train_examples+num_val_examples]
list_tok_seq_test, list_mask_seq_test, target_labels_test = \
    list_tok_seq_all[num_train_examples+num_val_examples:], list_mask_seq_all[num_train_examples+num_val_examples:], target_labels_all[num_train_examples+num_val_examples:]

# PyTorch 텐서로 변환
train_tok, train_mask, train_y = map(lambda x: torch.tensor(x, dtype=torch.long), [list_tok_seq_train, list_mask_seq_train, target_labels_train])
val_tok, val_mask, val_y = map(lambda x: torch.tensor(x, dtype=torch.long), [list_tok_seq_val, list_mask_seq_val, target_labels_val])
test_tok, test_mask, test_y = map(lambda x: torch.tensor(x, dtype=torch.long), [list_tok_seq_test, list_mask_seq_test, target_labels_test])

# 출력
print("Number of examples (total, train, validation, test):", total_num_examples, num_train_examples, num_val_examples, num_test_examples)

# @title 데이터로더 및 샘플러 선언

train_data = TensorDataset(train_tok, train_mask, train_y)
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size, drop_last=True)

val_data = TensorDataset(val_tok, val_mask, val_y)
val_sampler = SequentialSampler(val_data)
val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size, drop_last=True)

test_data = TensorDataset(test_tok, test_mask, test_y)
test_sampler = SequentialSampler(test_data)
test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size, drop_last=True)

# @title 모델 구조 선언

class BERT_Arch(nn.Module):
    def __init__(self, bert):
        super(BERT_Arch, self).__init__()
        self.bert = bert
        self.dropout = nn.Dropout(0.1)
        self.relu = nn.ReLU()
        self.fc1 = nn.Linear(768, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256,2)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, sent_id, mask):
        outputs = self.bert(input_ids=sent_id, attention_mask=mask)
        cls_out = outputs.pooler_output # [CLS]에 해당하는, 첫 번째 스텝의 출력만 추출
        x = self.fc1(cls_out)
        x1 = self.relu(x)
        x2 = self.dropout(x1)
        x3 = self.fc2(x2)
        x4 = self.relu(x3)
        x5 = self.dropout(x4)
        x6 = self.fc3(x5)
        x7 = self.softmax(x6)
        return x7

from tqdm.notebook import tqdm

# @title 모델 훈련, 검증, 최적 모델 저장 함수 선언
def train_model(model:nn.Module, epochs:int, train_dataloader:DataLoader, val_dataloader:DataLoader)->None:
    """Trains a PyTorch model and saves the best performing model weights.

    This function trains the given PyTorch model using the provided training and validation
    dataloaders for a specified number of epochs. It evaluates the model's performance on
    the validation set after each epoch and saves the model weights if the accuracy improves.

    Args:
        model (nn.Module): The PyTorch model to be trained.
        epochs (int): The number of training epochs.
        train_dataloader (DataLoader): The DataLoader for the training data.
        val_dataloader (DataLoader): The DataLoader for the validation data.
    """
    best_acc = 0.0
    num_training_batches = len(train_dataloader)
    num_val_batches = len(val_dataloader)
    print(f"Number of train and validation batches: {num_training_batches}, {num_val_batches}")
    for epoch in range(epochs):
        # Training phase
        model.train()
        total_loss, total_accuracy = 0, 0

        with tqdm(total=num_training_batches, desc=f"Epoch {epoch + 1}/{epochs}", leave=False) as pbar:
            for step, batch in enumerate(train_dataloader):
                batch = [r.to(device) for r in batch]
                sent_id, mask, labels = batch

                model.zero_grad()
                preds = model(sent_id, mask)
                loss = loss_fn(preds, labels)

                total_loss += loss.item()
                loss.backward()
                optimizer.step()

                # Update progress bar
                pbar.update(1)
                pbar.set_postfix({"Loss": f"{loss.item():.4f}"})

        avg_loss = total_loss / num_training_batches
        print(f"Epoch {epoch + 1} completed. Average Loss: {avg_loss:.4f}")
        # evaluate model after each epoch
        model.eval()
        hit_cnt = 0
        total_cnt = 0

        with torch.no_grad():
            for step, batch in enumerate(val_dataloader):
                batch = [r.to(device) for r in batch]
                tok_part, mask_part, y_part = batch
                # model prediction
                preds = model(tok_part, mask_part)

                preds_label = torch.argmax(preds, dim=1)
                preds_label = preds_label.cpu()
                preds_label = preds_label.numpy()
                y_part = y_part.cpu()
                y_part = y_part.numpy()

                for i in range(y_part.shape[0]):
                    if preds_label[i] == y_part[i]:
                        hit_cnt += 1
                total_cnt += len(y_part)
            acc = hit_cnt/total_cnt
            print("At epoch:", epoch, " avg loss=", avg_loss, " Accuracy = ", acc)
        # save the best model if the model has improved.
        if acc > best_acc:
            best_acc = acc
            torch.save(model.state_dict(), 'saved_weights.pt')
            print("Model parameters are saved.")
    print("Training complete!")
    print()

# @title 실제 훈련
for epochs in [7, 10, 12, 14]:
    model = BERT_Arch(bert)
    model = model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)
    loss_fn = nn.NLLLoss()
    num_training_batches = len(train_dataloader)
    num_val_batches = len(val_dataloader)

    train_model(model=model, epochs=epochs,
                train_dataloader=train_dataloader,
                val_dataloader=val_dataloader)

# @title 최적 모델로 테스트

## 여기에 test 단계를 넣어야 한다.
model.eval()
hit_cnt = 0
total_cnt = 0
test_loss = 0

model = BERT_Arch(bert)
model.load_state_dict(torch.load('saved_weights.pt'))
model = model.to(device)

with torch.no_grad():
    for step, batch in enumerate(test_dataloader):
        batch = [r.to(device) for r in batch]
        tok_part, mask_part, y_part = batch
        # model prediction
        preds = model(tok_part, mask_part)

        loss = loss_fn(preds, y_part)
        test_loss += loss.item()
        avg_loss = test_loss / len(test_dataloader)

        preds_label = torch.argmax(preds, dim=1)
        preds_label = preds_label.cpu()
        preds_label = preds_label.numpy()
        y_part = y_part.cpu()
        y_part = y_part.numpy()

        for i in range(y_part.shape[0]):
            if preds_label[i] == y_part[i]:
                hit_cnt += 1
        total_cnt += len(y_part)
    acc = hit_cnt/total_cnt
    print("Test Loss = ", test_loss, "Test Accuracy = ", acc)
print("Program ends.")